from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import json
import os
import requests


def load_and_process_experiment_results(file_path: str) -> Tuple[Dict[str, Dict[str, Dict[str, List[int]]]], Dict[str, Dict[str, List[str]]]]:
    """
    Load experiment results from JSON file generated by main.py and convert to analytics format.
    
    Args:
        file_path: Path to the JSON file generated by main.py
        
    Returns:
        Tuple of (search_analytics_data, ai_response_chunks)
        - search_analytics_data: Format expected by SearchAnalytics class
        - ai_response_chunks: Placeholder for response chunks (empty for now)
    """
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Handle the new format - data is now a list of experiment results
        if not isinstance(data, list):
            data = [data]  # Convert single result to list for backward compatibility
        
        # Aggregate all web searches from all experiments, models, and runs
        aggregated_searches = defaultdict(lambda: defaultdict(list))
        
        for experiment in data:
            prompt = experiment.get('prompt', '')
            results = experiment.get('results', {})
            
            for model_name, model_runs in results.items():
                for run_result in model_runs:
                    if run_result.get('success', False) and 'web_searches' in run_result:
                        web_searches = run_result['web_searches']
                        
                        # Handle the new format where web_searches is a dict of query -> domain -> citations_dict
                        if isinstance(web_searches, dict):
                            for query, domains in web_searches.items():
                                if isinstance(domains, dict):
                                    for domain, citations_data in domains.items():
                                        if isinstance(citations_data, dict):
                                            # New format: {"citations": [...], "contents": [...]}
                                            citations = citations_data.get('citations', [])
                                            if isinstance(citations, list):
                                                aggregated_searches[query][domain].extend(citations)
                                        elif isinstance(citations_data, list):
                                            # Old format: just a list of citations
                                            aggregated_searches[query][domain].extend(citations_data)
        
        # Convert to the expected format - group by prompt
        search_analytics_data = {}
        
        for experiment in data:
            prompt = experiment.get('prompt', '')
            if prompt not in search_analytics_data:
                search_analytics_data[prompt] = {}
            
            # Process this experiment's searches
            for query, domains in aggregated_searches.items():
                if query not in search_analytics_data[prompt]:
                    search_analytics_data[prompt][query] = {}
                
                for domain, citations in domains.items():
                    # Remove duplicates and sort
                    unique_citations = sorted(list(set(citations)))
                    if domain not in search_analytics_data[prompt][query]:
                        search_analytics_data[prompt][query][domain] = []
                    search_analytics_data[prompt][query][domain].extend(unique_citations)
        
        # Clean up duplicates in final result
        for prompt_data in search_analytics_data.values():
            for query_data in prompt_data.values():
                for domain in query_data:
                    query_data[domain] = sorted(list(set(query_data[domain])))
        
        # For now, return empty response chunks since main.py doesn't generate them
        ai_response_chunks = {}
        
        return search_analytics_data, ai_response_chunks
        
    except Exception as e:
        print(f"Error loading experiment results from {file_path}: {e}")
        return {}, {}


def load_multiple_experiment_files(file_paths: List[str]) -> Tuple[Dict[str, Dict[str, Dict[str, List[int]]]], Dict[str, Dict[str, List[str]]]]:
    """
    Load multiple experiment result files and combine them.
    
    Args:
        file_paths: List of paths to JSON files generated by main.py
        
    Returns:
        Tuple of (combined_search_analytics_data, combined_ai_response_chunks)
    """
    combined_search_data = {}
    combined_response_chunks = {}
    
    for file_path in file_paths:
        search_data, response_chunks = load_and_process_experiment_results(file_path)
        
        # Merge search data
        for prompt, queries in search_data.items():
            if prompt not in combined_search_data:
                combined_search_data[prompt] = {}
            
            for query, domains in queries.items():
                if query not in combined_search_data[prompt]:
                    combined_search_data[prompt][query] = {}
                
                for domain, citations in domains.items():
                    if domain not in combined_search_data[prompt][query]:
                        combined_search_data[prompt][query][domain] = []
                    combined_search_data[prompt][query][domain].extend(citations)
        
        # Merge response chunks
        combined_response_chunks.update(response_chunks)
    
    # Clean up duplicates in final result
    for prompt_data in combined_search_data.values():
        for query_data in prompt_data.values():
            for domain in query_data:
                query_data[domain] = sorted(list(set(query_data[domain])))
    
    return combined_search_data, combined_response_chunks


def analyze_experiment_results(file_path: str, domain_of_interest: str = None) -> Dict[str, Any]:
    """
    Analyze experiment results from a single file.
    
    Args:
        file_path: Path to the JSON file generated by main.py
        domain_of_interest: Optional domain to focus analysis on
        
    Returns:
        Dictionary containing analysis results
    """
    search_data, response_chunks = load_and_process_experiment_results(file_path)
    
    if not search_data:
        return {"error": "No valid data found in the file"}
    
    # Initialize analytics with the loaded data
    analytics = SearchAnalytics(search_data, response_chunks)
    
    # Generate comprehensive report
    report = analytics.generate_comprehensive_report(domain_of_interest)
    
    return report


def analyze_multiple_experiment_files(file_paths: List[str], domain_of_interest: str = None) -> Dict[str, Any]:
    """
    Analyze multiple experiment result files.
    
    Args:
        file_paths: List of paths to JSON files generated by main.py
        domain_of_interest: Optional domain to focus analysis on
        
    Returns:
        Dictionary containing combined analysis results
    """
    search_data, response_chunks = load_multiple_experiment_files(file_paths)
    
    if not search_data:
        return {"error": "No valid data found in the files"}
    
    # Initialize analytics with the combined data
    analytics = SearchAnalytics(search_data, response_chunks)
    
    # Generate comprehensive report
    report = analytics.generate_comprehensive_report(domain_of_interest)
    
    return report


def demo_with_experiment_file(file_path: str, domain_of_interest: str = None):
    """
    Demonstrate the analytics system with experiment file from main.py
    
    Args:
        file_path: Path to the JSON file generated by main.py
        domain_of_interest: Optional domain to focus analysis on
    """
    
    # Check if API key is available
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("üîë To see Gemini API analysis, set your GEMINI_API_KEY environment variable:")
        print("export GEMINI_API_KEY='your-api-key-here'")
        print("\nRunning without API key (will show error messages for Gemini calls)...")
        print("=" * 60)
    else:
        print("‚úÖ GEMINI_API_KEY found! Will provide AI-powered competitor analysis.")
        print("=" * 60)

    # Load and process the experiment results
    search_data, response_chunks = load_and_process_experiment_results(file_path)
    
    if not search_data:
        print(f"‚ùå Error: Could not load data from {file_path}")
        return
    
    # Initialize analytics with the loaded data
    analytics = SearchAnalytics(search_data, response_chunks)
    
    # Get the first (and likely only) prompt from the data
    prompt = list(search_data.keys())[0]
    
    # If no domain specified, try to guess from the data
    if not domain_of_interest:
        # Look for domains that appear frequently
        domain_counts = defaultdict(int)
        for queries in search_data.values():
            for domains in queries.values():
                for domain in domains.keys():
                    domain_counts[domain] += 1
        
        if domain_counts:
            domain_of_interest = max(domain_counts, key=domain_counts.get)
            print(f"ü§ñ Auto-detected domain of interest: {domain_of_interest}")
        else:
            print("‚ùå No domains found in the data")
            return
    
    print(f"\nüéØ ANALYZING DOMAIN: {domain_of_interest}")
    print("=" * 60)
    print(f"üìÑ Data loaded from: {file_path}")
    print(f"üìù Prompt: {prompt}")
    print("=" * 60)
    
    # Analyze the loaded data
    analytics.print_domain_analysis(domain_of_interest)
    
    # Analyze intersecting queries
    analytics.print_intersecting_queries_analysis()
    
    # Generate comprehensive report
    report = analytics.generate_comprehensive_report(domain_of_interest)
    
    print(f"\nüí° KEY INSIGHTS FOR {domain_of_interest}")
    print("=" * 60)
    
    print(f"üìà Overview:")
    print(f"   ‚Ä¢ Total Prompts: {report['overview']['total_prompts']}")
    print(f"   ‚Ä¢ Total Queries: {report['overview']['total_queries']}")
    print(f"   ‚Ä¢ Unique Domains: {report['overview']['total_unique_domains']}")
    
    if 'domain_analysis' in report:
        domain_stats = report['domain_analysis']
        print(f"   ‚Ä¢ Domain Retrieval Rate: {domain_stats['retrieval_rate']:.2%}")
        print(f"   ‚Ä¢ Domain Usage Rate: {domain_stats['usage_rate']:.2%}")
        print(f"   ‚Ä¢ Average Citation Rank: {domain_stats['avg_citation_rank']:.2f}")
        print(f"   ‚Ä¢ Total Citations: {domain_stats['total_citations']}")
    
    print(f"\nüìã RECOMMENDATIONS:")
    print(f"   ‚Ä¢ Analyze query patterns to identify optimization opportunities")
    print(f"   ‚Ä¢ Focus on improving performance in underperforming queries")
    print(f"   ‚Ä¢ Consider content optimization to improve citation rankings")


class SearchAnalytics:
    def __init__(self, data: Dict[str, Dict[str, Dict[str, List[int]]]], response_chunks: Dict[str, Dict[str, List[str]]] = None):
        self.data = data
        self.response_chunks = response_chunks or {}
        self.domain_stats = {}
        self.query_stats = {}
        self.prompt_stats = {}
        
    def calculate_domain_stats(self, domain_of_interest: str) -> Dict[str, Any]:
        """
        Calculate statistics for a specific domain of interest
        """
        stats = {
            'total_appearances': 0,
            'total_citations': 0,
            'retrieval_rate': 0.0,
            'usage_rate': 0.0,
            'avg_citation_rank': 0.0,
            'min_citation_rank': float('inf'),
            'prompt_appearances': [],
            'query_appearances': [],
            'citation_positions': []
        }
        
        total_queries = 0
        queries_with_domain = 0
        queries_with_citations = 0
        
        for prompt, queries in self.data.items():
            domain_in_prompt = False
            for query, domains in queries.items():
                total_queries += 1
                
                if domain_of_interest in domains:
                    queries_with_domain += 1
                    domain_in_prompt = True
                    stats['query_appearances'].append(query)
                    
                    citations = domains[domain_of_interest]
                    if citations:  # If domain was actually cited
                        queries_with_citations += 1
                        stats['total_citations'] += len(citations)
                        stats['citation_positions'].extend(citations)
                        
                        min_citation = min(citations)
                        if min_citation < stats['min_citation_rank']:
                            stats['min_citation_rank'] = min_citation
            
            if domain_in_prompt:
                stats['prompt_appearances'].append(prompt)
        
        stats['total_appearances'] = queries_with_domain
        stats['retrieval_rate'] = queries_with_domain / total_queries if total_queries > 0 else 0
        stats['usage_rate'] = queries_with_citations / queries_with_domain if queries_with_domain > 0 else 0
        
        if stats['citation_positions']:
            stats['avg_citation_rank'] = sum(stats['citation_positions']) / len(stats['citation_positions'])
        
        if stats['min_citation_rank'] == float('inf'):
            stats['min_citation_rank'] = None
            
        return stats
    
    def calculate_query_frequency_stats(self) -> Dict[str, Any]:
        """
        Calculate frequency statistics for all queries
        """
        query_counter = Counter()
        query_prompt_mapping = defaultdict(list)
        
        total_queries = 0
        
        for prompt, queries in self.data.items():
            for query in queries.keys():
                query_counter[query] += 1
                query_prompt_mapping[query].append(prompt)
                total_queries += 1
        
        query_stats = {}
        for query, count in query_counter.items():
            query_stats[query] = {
                'frequency': count,
                'frequency_rate': count / total_queries,
                'prompts': query_prompt_mapping[query],
                'unique_prompts': len(set(query_prompt_mapping[query]))
            }
        
        return {
            'total_queries': total_queries,
            'unique_queries': len(query_counter),
            'query_details': query_stats,
            'most_common_queries': query_counter.most_common(10)
        }
    
    def calculate_prompt_stats(self) -> Dict[str, Any]:
        """
        Calculate statistics for each prompt
        """
        prompt_stats = {}
        
        for prompt, queries in self.data.items():
            stats = {
                'total_queries': len(queries),
                'unique_domains': set(),
                'total_citations': 0,
                'domains_with_citations': set(),
                'avg_citations_per_query': 0.0
            }
            
            total_citations = 0
            
            for query, domains in queries.items():
                stats['unique_domains'].update(domains.keys())
                
                for domain, citations in domains.items():
                    if citations:
                        stats['domains_with_citations'].add(domain)
                        total_citations += len(citations)
            
            stats['total_citations'] = total_citations
            stats['avg_citations_per_query'] = total_citations / len(queries) if queries else 0
            stats['unique_domains'] = len(stats['unique_domains'])
            stats['domains_with_citations'] = len(stats['domains_with_citations'])
            
            prompt_stats[prompt] = stats
        
        return prompt_stats
    
    def generate_comprehensive_report(self, domain_of_interest: str = None) -> Dict[str, Any]:
        """
        Generate a comprehensive analytics report
        """
        report = {
            'overview': {
                'total_prompts': len(self.data),
                'total_queries': sum(len(queries) for queries in self.data.values()),
                'total_unique_domains': len(set(
                    domain for queries in self.data.values() 
                    for domains in queries.values() 
                    for domain in domains.keys()
                ))
            },
            'query_frequency_stats': self.calculate_query_frequency_stats(),
            'prompt_stats': self.calculate_prompt_stats()
        }
        
        if domain_of_interest:
            report['domain_analysis'] = self.calculate_domain_stats(domain_of_interest)
        
        return report
    
    def print_domain_analysis(self, domain: str):
        """
        Print a formatted analysis for a specific domain
        """
        stats = self.calculate_domain_stats(domain)
        
        print(f"\n=== DOMAIN ANALYSIS: {domain} ===")
        print(f"Total Appearances: {stats['total_appearances']}")
        print(f"Retrieval Rate: {stats['retrieval_rate']:.2%}")
        print(f"Usage Rate: {stats['usage_rate']:.2%}")
        print(f"Average Citation Rank: {stats['avg_citation_rank']:.2f}")
        print(f"Best Citation Rank: {stats['min_citation_rank']}")
        print(f"Total Citations: {stats['total_citations']}")
        
        print(f"\nPrompts where domain appeared:")
        for prompt in stats['prompt_appearances']:
            print(f"  - {prompt}")
        
        print(f"\nQueries where domain appeared:")
        for query in stats['query_appearances']:
            print(f"  - {query}")

    def analyze_intersecting_queries(self) -> Dict[str, Any]:
        """
        Analyze queries that appear across multiple prompts
        """
        query_prompt_mapping = defaultdict(list)
        query_domain_stats = defaultdict(lambda: defaultdict(set))
        
        # Map queries to prompts and track domains
        for prompt, queries in self.data.items():
            for query, domains in queries.items():
                query_prompt_mapping[query].append(prompt)
                for domain, citations in domains.items():
                    if citations:  # Only count domains that were actually cited
                        query_domain_stats[query][domain].update(citations)
        
        # Identify intersecting queries (appearing in multiple prompts)
        intersecting_queries = {
            query: prompts for query, prompts in query_prompt_mapping.items() 
            if len(set(prompts)) > 1
        }
        
        intersecting_analysis = {}
        for query, prompts in intersecting_queries.items():
            unique_prompts = list(set(prompts))
            intersecting_analysis[query] = {
                'frequency': len(prompts),
                'unique_prompts': len(unique_prompts),
                'prompts': unique_prompts,
                'total_domains': len(query_domain_stats[query]),
                'domains_with_citations': {
                    domain: list(citations) for domain, citations in query_domain_stats[query].items()
                }
            }
        
        return intersecting_analysis
    
    def print_intersecting_queries_analysis(self):
        """
        Print detailed analysis of intersecting queries
        """
        intersecting = self.analyze_intersecting_queries()
        
        print(f"\n=== INTERSECTING QUERIES ANALYSIS ===")
        print(f"Total Intersecting Queries: {len(intersecting)}")
        
        for query, stats in intersecting.items():
            print(f"\nüìä Query: '{query}'")
            print(f"   ‚Ä¢ Appears in {stats['frequency']} searches across {stats['unique_prompts']} prompts")
            print(f"   ‚Ä¢ Total domains with citations: {stats['total_domains']}")
            
            print(f"   ‚Ä¢ Prompts:")
            for i, prompt in enumerate(stats['prompts'], 1):
                short_prompt = prompt[:80] + "..." if len(prompt) > 80 else prompt
                print(f"     {i}. {short_prompt}")
            
            print(f"   ‚Ä¢ Top domains across all instances:")
            domain_totals = {}
            for domain, citations in stats['domains_with_citations'].items():
                domain_totals[domain] = len(citations)
            
            # Sort domains by total citations
            sorted_domains = sorted(domain_totals.items(), key=lambda x: x[1], reverse=True)
            for domain, citation_count in sorted_domains[:5]:  # Show top 5
                print(f"     - {domain}: {citation_count} citations")

    def analyze_response_chunks(self, domain_of_interest: str) -> Dict[str, Any]:
        """
        Analyze AI response chunks to understand domain performance in final responses
        """
        chunk_analysis = {}
        
        for prompt, chunks in self.response_chunks.items():
            domain_mentions = []
            competitor_mentions = []
            total_citations = 0
            
            for sentence, domains in chunks.items():
                total_citations += len(domains)
                
                if domain_of_interest in domains:
                    domain_mentions.append({
                        'sentence': sentence,
                        'position': len(domain_mentions) + 1,
                        'citation_count': domains.count(domain_of_interest)
                    })
                else:
                    # Track competitors mentioned in this sentence
                    for domain in domains:
                        competitor_mentions.append({
                            'sentence': sentence,
                            'domain': domain,
                            'position': len(competitor_mentions) + 1
                        })
            
            # Calculate performance metrics
            domain_citations = sum(mention['citation_count'] for mention in domain_mentions)
            competitor_citations = len(competitor_mentions)
            
            chunk_analysis[prompt] = {
                'domain_mentions': domain_mentions,
                'competitor_mentions': competitor_mentions,
                'domain_citations': domain_citations,
                'competitor_citations': competitor_citations,
                'total_citations': total_citations,
                'domain_share': domain_citations / total_citations if total_citations > 0 else 0,
                'first_mention_position': domain_mentions[0]['position'] if domain_mentions else None,
                'performance_rating': self._rate_performance(domain_mentions, competitor_mentions)
            }
        
        return chunk_analysis
    
    def _rate_performance(self, domain_mentions: List[Dict], competitor_mentions: List[Dict]) -> str:
        """Rate domain performance as 'excellent', 'good', 'poor', or 'absent'"""
        if not domain_mentions:
            return 'absent'
        
        first_mention = domain_mentions[0]['position']
        total_citations = sum(mention['citation_count'] for mention in domain_mentions)
        
        if first_mention <= 2 and total_citations >= 2:
            return 'excellent'
        elif first_mention <= 3 and total_citations >= 1:
            return 'good'
        else:
            return 'poor'
    
    def call_gemini_analysis(self, prompt: str, domain_of_interest: str, competitor_sentences: List[str]) -> str:
        """
        Call Gemini API to analyze why competitors ranked higher
        """
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            return "ERROR: GEMINI_API_KEY not found in environment variables"
        
        # Prepare the analysis prompt for Gemini
        analysis_prompt = f"""
        Analyze why competing domains ranked higher than {domain_of_interest} in this AI search response.

        User Query: {prompt}

        Competitor statements that ranked higher:
        {chr(10).join(f"- {sentence}" for sentence in competitor_sentences)}

        Please briefly explain (2-3 sentences) what key advantages or features these competitors demonstrated that made them rank higher than {domain_of_interest}. Focus on specific capabilities, market position, or features mentioned.
        """
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-latest:generateContent?key={api_key}"
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": analysis_prompt
                }]
            }]
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            result = response.json()
            if 'candidates' in result and len(result['candidates']) > 0:
                return result['candidates'][0]['content']['parts'][0]['text']
            else:
                return "ERROR: No response from Gemini API"
                
        except requests.RequestException as e:
            return f"ERROR: API request failed - {str(e)}"
        except (KeyError, IndexError) as e:
            return f"ERROR: Invalid response format - {str(e)}"
    
    def analyze_poor_performance(self, domain_of_interest: str) -> Dict[str, Any]:
        """
        Identify prompts where domain performed poorly and get Gemini analysis
        """
        chunk_analysis = self.analyze_response_chunks(domain_of_interest)
        poor_performance_analysis = {}
        
        for prompt, analysis in chunk_analysis.items():
            if analysis['performance_rating'] in ['poor', 'absent']:
                # Get competitor sentences that ranked higher
                competitor_sentences = []
                for competitor in analysis['competitor_mentions'][:5]:  # Top 5 competitors
                    competitor_sentences.append(competitor['sentence'])
                
                if competitor_sentences:
                    gemini_analysis = self.call_gemini_analysis(
                        prompt, 
                        domain_of_interest, 
                        competitor_sentences
                    )
                    
                    poor_performance_analysis[prompt] = {
                        'performance_rating': analysis['performance_rating'],
                        'domain_citations': analysis['domain_citations'],
                        'competitor_citations': analysis['competitor_citations'],
                        'domain_share': analysis['domain_share'],
                        'competitor_sentences': competitor_sentences,
                        'gemini_analysis': gemini_analysis
                    }
        
        return poor_performance_analysis
    
    def print_poor_performance_analysis(self, domain_of_interest: str):
        """
        Print detailed analysis of poor performance cases with Gemini insights
        """
        poor_performance = self.analyze_poor_performance(domain_of_interest)
        
        if not poor_performance:
            print(f"\nüéâ No poor performance cases found for {domain_of_interest}!")
            return
        
        print(f"\nüîç POOR PERFORMANCE ANALYSIS FOR {domain_of_interest}")
        print("=" * 80)
        
        for prompt, analysis in poor_performance.items():
            short_prompt = prompt[:100] + "..." if len(prompt) > 100 else prompt
            print(f"\nüìâ Prompt: {short_prompt}")
            print(f"   Performance Rating: {analysis['performance_rating'].upper()}")
            print(f"   Domain Citations: {analysis['domain_citations']}")
            print(f"   Competitor Citations: {analysis['competitor_citations']}")
            print(f"   Domain Share: {analysis['domain_share']:.2%}")
            
            print(f"\n   ü§ñ Gemini Analysis:")
            print(f"   {analysis['gemini_analysis']}")
            
            print(f"\n   üèÜ Top Competitor Statements:")
            for i, sentence in enumerate(analysis['competitor_sentences'][:3], 1):
                print(f"   {i}. {sentence}")
            print()


# Legacy demo function - now uses hardcoded data for backward compatibility
def demo_with_api():
    """
    Demonstrate the analytics system with hardcoded data (for backward compatibility)
    """
    print("‚ö†Ô∏è  This function uses hardcoded demo data.")
    print("üí° For real experiment analysis, use demo_with_experiment_file() instead.")
    print("=" * 60)
    
    # Create some sample data for demonstration
    sample_data = {
        "What are the best AI SEO tools?": {
            "best ai seo tools": {
                "https://peeq.ai": [1, 3],
                "https://surfer.com": [2],
                "https://marketmuse.com": [4, 5]
            },
            "ai content optimization": {
                "https://peeq.ai": [1],
                "https://surfer.com": [2, 3],
                "https://clearscope.io": [4]
            }
        }
    }
    
    analytics = SearchAnalytics(sample_data, {})
    
    print("\nüéØ SAMPLE ANALYSIS")
    print("=" * 40)
    
    # Analyze a sample domain
    domain_of_interest = "https://peeq.ai"
    analytics.print_domain_analysis(domain_of_interest)
    
    # Generate report
    report = analytics.generate_comprehensive_report(domain_of_interest)
    print(f"\nüìä SAMPLE REPORT")
    print(f"Total Prompts: {report['overview']['total_prompts']}")
    print(f"Total Queries: {report['overview']['total_queries']}")
    print(f"Unique Domains: {report['overview']['total_unique_domains']}")


def main():
    """
    Main function - checks for experiment results file, otherwise shows demo
    """
    # Check if experiment results file exists (new filename)
    experiment_file = "internal_responce_log.json"
    
    if os.path.exists(experiment_file):
        print(f"üìÅ Found experiment results file: {experiment_file}")
        print("üîç Running analysis on experiment data...")
        print("=" * 60)
        
        # Run analysis on the experiment file
        demo_with_experiment_file(experiment_file)
    else:
        print(f"üìÅ No experiment results file found: {experiment_file}")


if __name__ == "__main__":
    main()
