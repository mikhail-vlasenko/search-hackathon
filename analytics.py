from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import json
import os
import requests
from urllib.parse import urlparse


def extract_domain_from_url(url: str) -> str:
    """
    Extract domain name from a URL.
    
    Args:
        url: Full URL (e.g., "https://radwelt.berlin/fahrrad-kaufen/gebrauchte-fahrraeder-berlin")
        
    Returns:
        Domain name (e.g., "radwelt.berlin")
    """
    try:
        if not url:
            return ""
        
        # Handle URLs that don't have a scheme
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # Remove www. prefix if present
        if domain.startswith('www.'):
            domain = domain[4:]
        
        return domain
    except Exception:
        # If URL parsing fails, return the original URL cleaned up
        return url.replace('https://', '').replace('http://', '').replace('www.', '').split('/')[0].lower()


def load_and_process_experiment_results(file_path: str) -> Tuple[Dict[str, Dict[str, Dict[str, Dict[str, Any]]]], Dict[str, Dict[str, List[str]]]]:
    """
    Load experiment results from JSON file generated by main.py and convert to analytics format.
    
    Args:
        file_path: Path to the JSON file generated by main.py
        
    Returns:
        Tuple of (search_analytics_data, ai_response_chunks)
        - search_analytics_data: Format expected by SearchAnalytics class with both citations and contents
        - ai_response_chunks: Placeholder for response chunks (empty for now)
    """
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Handle the new format - data is now a list of experiment results
        if not isinstance(data, list):
            data = [data]  # Convert single result to list for backward compatibility
        
        # Process each experiment separately to maintain prompt-query associations
        search_analytics_data = {}
        
        for experiment in data:
            prompt = experiment.get('prompt', '')
            results = experiment.get('results', {})
            
            if prompt not in search_analytics_data:
                search_analytics_data[prompt] = {}
            
            # Aggregate searches for this specific prompt only
            prompt_searches = defaultdict(lambda: defaultdict(lambda: {'citations': [], 'contents': []}))
            
            for model_name, model_runs in results.items():
                for run_result in model_runs:
                    if run_result.get('success', False) and 'web_searches' in run_result:
                        web_searches = run_result['web_searches']
                        
                        # Handle the new format where web_searches is a dict of query -> domain -> citations_dict
                        if isinstance(web_searches, dict):
                            for query, domains in web_searches.items():
                                if isinstance(domains, dict):
                                    for domain_url, citations_data in domains.items():
                                        # Extract domain name from URL
                                        domain = extract_domain_from_url(domain_url)
                                        
                                        if isinstance(citations_data, dict):
                                            # New format: {"citations": [...], "contents": [...]}
                                            citations = citations_data.get('citations', [])
                                            contents = citations_data.get('contents', [])
                                            if isinstance(citations, list):
                                                prompt_searches[query][domain]['citations'].extend(citations)
                                            if isinstance(contents, list):
                                                prompt_searches[query][domain]['contents'].extend(contents)
                                        elif isinstance(citations_data, list):
                                            # Old format: just a list of citations
                                            prompt_searches[query][domain]['citations'].extend(citations_data)
            
            # Convert to the expected format for this prompt
            for query, domains in prompt_searches.items():
                if query not in search_analytics_data[prompt]:
                    search_analytics_data[prompt][query] = {}
                
                for domain, citation_data in domains.items():
                    if domain not in search_analytics_data[prompt][query]:
                        search_analytics_data[prompt][query][domain] = {
                            'citations': [],
                            'contents': []
                        }
                    
                    # Remove duplicates and sort citations
                    unique_citations = sorted(list(set(citation_data['citations'])))
                    search_analytics_data[prompt][query][domain]['citations'] = unique_citations
                    
                    # Add contents (may have duplicates, but that's okay for analysis)
                    search_analytics_data[prompt][query][domain]['contents'] = citation_data['contents']
        
        # Clean up duplicates in final result (already done above, but keeping for consistency)
        for prompt_data in search_analytics_data.values():
            for query_data in prompt_data.values():
                for domain_data in query_data.values():
                    domain_data['citations'] = sorted(list(set(domain_data['citations'])))
                    # Keep contents as is - duplicates might be meaningful
        
        # Extract AI response chunks from experiment results
        ai_response_chunks = {}
        
        for experiment in data:
            prompt = experiment.get('prompt', '')
            results = experiment.get('results', {})
            
            if prompt not in ai_response_chunks:
                ai_response_chunks[prompt] = {}
            
            for model_name, model_runs in results.items():
                for run_result in model_runs:
                    if run_result.get('success', False) and 'response' in run_result:
                        response_text = run_result['response']
                        if response_text:
                            # Split response into chunks (sentences or paragraphs)
                            # For now, split by periods followed by space or newline
                            chunks = [chunk.strip() for chunk in response_text.split('.') if chunk.strip()]
                            
                            # Store chunks for this prompt/model combination
                            chunk_key = f"{model_name}_response"
                            if chunk_key not in ai_response_chunks[prompt]:
                                ai_response_chunks[prompt][chunk_key] = []
                            ai_response_chunks[prompt][chunk_key].extend(chunks)
        
        return search_analytics_data, ai_response_chunks
        
    except Exception as e:
        print(f"Error loading experiment results from {file_path}: {e}")
        return {}, {}


def load_multiple_experiment_files(file_paths: List[str]) -> Tuple[Dict[str, Dict[str, Dict[str, Dict[str, Any]]]], Dict[str, Dict[str, List[str]]]]:
    """
    Load multiple experiment result files and combine them.
    
    Args:
        file_paths: List of paths to JSON files generated by main.py
        
    Returns:
        Tuple of (combined_search_analytics_data, combined_ai_response_chunks)
    """
    combined_search_data = {}
    combined_response_chunks = {}
    
    for file_path in file_paths:
        search_data, response_chunks = load_and_process_experiment_results(file_path)
        
        # Merge search data
        for prompt, queries in search_data.items():
            if prompt not in combined_search_data:
                combined_search_data[prompt] = {}
            
            for query, domains in queries.items():
                if query not in combined_search_data[prompt]:
                    combined_search_data[prompt][query] = {}
                
                for domain_url, citation_data in domains.items():
                    # Extract domain name from URL (in case it hasn't been processed yet)
                    domain = extract_domain_from_url(domain_url)
                    
                    if domain not in combined_search_data[prompt][query]:
                        combined_search_data[prompt][query][domain] = {
                            'citations': [],
                            'contents': []
                        }
                    combined_search_data[prompt][query][domain]['citations'].extend(citation_data['citations'])
                    combined_search_data[prompt][query][domain]['contents'].extend(citation_data['contents'])
        
        # Merge response chunks
        for prompt, chunks in response_chunks.items():
            if prompt not in combined_response_chunks:
                combined_response_chunks[prompt] = {}
            
            for chunk_key, chunk_list in chunks.items():
                if chunk_key not in combined_response_chunks[prompt]:
                    combined_response_chunks[prompt][chunk_key] = []
                combined_response_chunks[prompt][chunk_key].extend(chunk_list)
    
    # Clean up duplicates in final result
    for prompt_data in combined_search_data.values():
        for query_data in prompt_data.values():
            for domain_data in query_data.values():
                domain_data['citations'] = sorted(list(set(domain_data['citations'])))
                # Keep contents as is - duplicates might be meaningful
    
    return combined_search_data, combined_response_chunks


def analyze_experiment_results(file_path: str, domain_of_interest: str = None) -> Dict[str, Any]:
    """
    Analyze experiment results from a single file.
    
    Args:
        file_path: Path to the JSON file generated by main.py
        domain_of_interest: Optional domain to focus analysis on
        
    Returns:
        Dictionary containing analysis results
    """
    search_data, response_chunks = load_and_process_experiment_results(file_path)
    
    if not search_data:
        return {"error": "No valid data found in the file"}
    
    # Initialize analytics with the loaded data
    analytics = SearchAnalytics(search_data, response_chunks)
    
    # Generate comprehensive report
    report = analytics.generate_comprehensive_report(domain_of_interest)
    
    return report


def analyze_multiple_experiment_files(file_paths: List[str], domain_of_interest: str = None) -> Dict[str, Any]:
    """
    Analyze multiple experiment result files.
    
    Args:
        file_paths: List of paths to JSON files generated by main.py
        domain_of_interest: Optional domain to focus analysis on
        
    Returns:
        Dictionary containing combined analysis results
    """
    search_data, response_chunks = load_multiple_experiment_files(file_paths)
    
    if not search_data:
        return {"error": "No valid data found in the files"}
    
    # Initialize analytics with the combined data
    analytics = SearchAnalytics(search_data, response_chunks)
    
    # Generate comprehensive report
    report = analytics.generate_comprehensive_report(domain_of_interest)
    
    return report


def demo_with_experiment_file(file_path: str, domain_of_interest: str = None):
    """
    Demonstrate the analytics system with experiment file from main.py
    
    Args:
        file_path: Path to the JSON file generated by main.py
        domain_of_interest: Optional domain to focus analysis on
    """
    
    # Check if API key is available
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("🔑 To see Gemini API analysis, set your GEMINI_API_KEY environment variable:")
        print("export GEMINI_API_KEY='your-api-key-here'")
        print("\nRunning without API key (will show error messages for Gemini calls)...")
        print("=" * 60)
    else:
        print("✅ GEMINI_API_KEY found! Will provide AI-powered competitor analysis.")
        print("=" * 60)

    # Load and process the experiment results
    search_data, response_chunks = load_and_process_experiment_results(file_path)
    
    if not search_data:
        print(f"❌ Error: Could not load data from {file_path}")
        return
    
    # Initialize analytics with the loaded data
    analytics = SearchAnalytics(search_data, response_chunks)
    
    # Get the first (and likely only) prompt from the data
    prompt = list(search_data.keys())[0]
    
    # If no domain specified, try to guess from the data
    if not domain_of_interest:
        # Look for domains that appear frequently
        domain_counts = defaultdict(int)
        for queries in search_data.values():
            for domains in queries.values():
                for domain in domains.keys():
                    domain_counts[domain] += 1
        
        if domain_counts:
            domain_of_interest = max(domain_counts, key=domain_counts.get)
            print(f"🤖 Auto-detected domain of interest: {domain_of_interest}")
        else:
            print("❌ No domains found in the data")
            return
    
    print(f"\n🎯 ANALYZING DOMAIN: {domain_of_interest}")
    print("=" * 60)
    print(f"📄 Data loaded from: {file_path}")
    print(f"📝 Prompt: {prompt}")
    print("=" * 60)
    
    # Analyze the loaded data
    analytics.print_domain_analysis(domain_of_interest)
    
    # Analyze intersecting queries
    analytics.print_intersecting_queries_analysis()
    
    # Print poor performance analysis with Gemini insights
    analytics.print_poor_performance_analysis(domain_of_interest)
    
    # Generate comprehensive report (now includes Gemini analysis)
    report = analytics.generate_comprehensive_report(domain_of_interest)
    
    print(f"\n💡 KEY INSIGHTS FOR {domain_of_interest}")
    print("=" * 60)
    
    print(f"📈 Overview:")
    print(f"   • Total Prompts: {report['overview']['total_prompts']}")
    print(f"   • Total Queries: {report['overview']['total_queries']}")
    print(f"   • Unique Domains: {report['overview']['total_unique_domains']}")
    
    if 'domain_analysis' in report:
        domain_stats = report['domain_analysis']
        print(f"   • Domain Retrieval Rate: {domain_stats['retrieval_rate']:.2%}")
        print(f"   • Domain Usage Rate: {domain_stats['usage_rate']:.2%}")
        print(f"   • Average Citation Rank: {domain_stats['avg_citation_rank']:.2f}")
        print(f"   • Total Citations: {domain_stats['total_citations']}")
    
    # Print Gemini insights summary if available
    if 'gemini_analysis' in report:
        gemini_data = report['gemini_analysis']
        print(f"\n🤖 GEMINI AI INSIGHTS:")
        print(f"   • Performance Issues Found: {len(gemini_data)}")
        if gemini_data:
            print(f"   • AI recommendations available for improvement strategies")
    


class SearchAnalytics:
    def __init__(self, data: Dict[str, Dict[str, Dict[str, Dict[str, Any]]]], response_chunks: Dict[str, Dict[str, List[str]]] = None):
        self.data = data
        self.response_chunks = response_chunks or {}
        self.domain_stats = {}
        self.query_stats = {}
        self.prompt_stats = {}
        
    def calculate_domain_stats(self, domain_of_interest: str) -> Dict[str, Any]:
        """
        Calculate statistics for a specific domain of interest
        """
        stats = {
            'total_appearances': 0,
            'total_citations': 0,
            'retrieval_rate': 0.0,
            'usage_rate': 0.0,
            'avg_citation_rank': 0.0,
            'min_citation_rank': float('inf'),
            'prompt_appearances': [],
            'query_appearances': [],
            'citation_positions': []
        }
        
        total_queries = 0
        queries_with_domain = 0
        queries_with_citations = 0
        
        for prompt, queries in self.data.items():
            domain_in_prompt = False
            for query, domains in queries.items():
                total_queries += 1
                
                if domain_of_interest in domains:
                    queries_with_domain += 1
                    domain_in_prompt = True
                    stats['query_appearances'].append(query)
                    
                    citations = domains[domain_of_interest]['citations']
                    if citations:  # If domain was actually cited
                        queries_with_citations += 1
                        stats['total_citations'] += len(citations)
                        stats['citation_positions'].extend(citations)
                        
                        min_citation = min(citations)
                        if min_citation < stats['min_citation_rank']:
                            stats['min_citation_rank'] = min_citation
            
            if domain_in_prompt:
                stats['prompt_appearances'].append(prompt)
        
        stats['total_appearances'] = queries_with_domain
        stats['retrieval_rate'] = queries_with_domain / total_queries if total_queries > 0 else 0
        stats['usage_rate'] = queries_with_citations / queries_with_domain if queries_with_domain > 0 else 0
        
        if stats['citation_positions']:
            stats['avg_citation_rank'] = sum(stats['citation_positions']) / len(stats['citation_positions'])
        else:
            stats['avg_citation_rank'] = None  # Set to None when no citations exist
        
        if stats['min_citation_rank'] == float('inf'):
            stats['min_citation_rank'] = None
            
        return stats
    
    def calculate_query_frequency_stats(self) -> Dict[str, Any]:
        """
        Calculate frequency statistics for all queries
        """
        query_counter = Counter()
        query_prompt_mapping = defaultdict(list)
        
        total_queries = 0
        
        for prompt, queries in self.data.items():
            for query in queries.keys():
                query_counter[query] += 1
                query_prompt_mapping[query].append(prompt)
                total_queries += 1
        
        query_stats = {}
        for query, count in query_counter.items():
            query_stats[query] = {
                'frequency': count,
                'frequency_rate': count / total_queries,
                'prompts': query_prompt_mapping[query],
                'unique_prompts': len(set(query_prompt_mapping[query]))
            }
        
        return {
            'total_queries': total_queries,
            'unique_queries': len(query_counter),
            'query_details': query_stats,
            'most_common_queries': query_counter.most_common(10)
        }
    
    def calculate_prompt_stats(self) -> Dict[str, Any]:
        """
        Calculate statistics for each prompt
        """
        prompt_stats = {}
        
        for prompt, queries in self.data.items():
            stats = {
                'total_queries': len(queries),
                'unique_domains': set(),
                'total_citations': 0,
                'domains_with_citations': set(),
                'avg_citations_per_query': 0.0
            }
            
            total_citations = 0
            
            for query, domains in queries.items():
                stats['unique_domains'].update(domains.keys())
                
                for domain, citation_data in domains.items():
                    if citation_data['citations']:
                        stats['domains_with_citations'].add(domain)
                        total_citations += len(citation_data['citations'])
            
            stats['total_citations'] = total_citations
            stats['avg_citations_per_query'] = total_citations / len(queries) if queries else 0
            stats['unique_domains'] = len(stats['unique_domains'])
            stats['domains_with_citations'] = len(stats['domains_with_citations'])
            
            prompt_stats[prompt] = stats
        
        return prompt_stats
    
    def generate_comprehensive_report(self, domain_of_interest: str = None) -> Dict[str, Any]:
        """
        Generate a comprehensive analytics report
        """
        report = {
            'overview': {
                'total_prompts': len(self.data),
                'total_queries': sum(len(queries) for queries in self.data.values()),
                'total_unique_domains': len(set(
                    domain for queries in self.data.values() 
                    for domains in queries.values() 
                    for domain in domains.keys()
                ))
            },
            'query_frequency_stats': self.calculate_query_frequency_stats(),
            'prompt_stats': self.calculate_prompt_stats()
        }
        
        if domain_of_interest:
            report['domain_analysis'] = self.calculate_domain_stats(domain_of_interest)
            
            # Add response chunks analysis if available
            if self.response_chunks:
                report['response_chunks_analysis'] = self.analyze_response_chunks(domain_of_interest)
                
            # Add Gemini analysis for poor performance cases
            report['gemini_analysis'] = self.analyze_poor_performance(domain_of_interest)
        
        return report
    
    def print_domain_analysis(self, domain: str):
        """
        Print a formatted analysis for a specific domain
        """
        stats = self.calculate_domain_stats(domain)
        
        print(f"\n=== DOMAIN ANALYSIS: {domain} ===")
        print(f"Total Appearances: {stats['total_appearances']}")
        print(f"Retrieval Rate: {stats['retrieval_rate']:.2%}")
        print(f"Usage Rate: {stats['usage_rate']:.2%}")
        avg_rank_str = f"{stats['avg_citation_rank']:.2f}" if stats['avg_citation_rank'] is not None else "No citations"
        print(f"Average Citation Rank: {avg_rank_str}")
        print(f"Best Citation Rank: {stats['min_citation_rank']}")
        print(f"Total Citations: {stats['total_citations']}")
        
        print(f"\nPrompts where domain appeared:")
        for prompt in stats['prompt_appearances']:
            print(f"  - {prompt}")
        
        print(f"\nQueries where domain appeared:")
        for query in stats['query_appearances']:
            print(f"  - {query}")

    def analyze_intersecting_queries(self) -> Dict[str, Any]:
        """
        Analyze queries that appear across multiple prompts
        """
        query_prompt_mapping = defaultdict(list)
        query_domain_stats = defaultdict(lambda: defaultdict(set))
        
        # Map queries to prompts and track domains
        for prompt, queries in self.data.items():
            for query, domains in queries.items():
                query_prompt_mapping[query].append(prompt)
                for domain, citation_data in domains.items():
                    if citation_data['citations']:  # Only count domains that were actually cited
                        query_domain_stats[query][domain].update(citation_data['citations'])
        
        # Identify intersecting queries (appearing in multiple prompts)
        intersecting_queries = {
            query: prompts for query, prompts in query_prompt_mapping.items() 
            if len(set(prompts)) > 1
        }
        
        intersecting_analysis = {}
        for query, prompts in intersecting_queries.items():
            unique_prompts = list(set(prompts))
            intersecting_analysis[query] = {
                'frequency': len(prompts),
                'unique_prompts': len(unique_prompts),
                'prompts': unique_prompts,
                'total_domains': len(query_domain_stats[query]),
                'domains_with_citations': {
                    domain: list(citations) for domain, citations in query_domain_stats[query].items()
                }
            }
        
        return intersecting_analysis
    
    def print_intersecting_queries_analysis(self):
        """
        Print detailed analysis of intersecting queries
        """
        intersecting = self.analyze_intersecting_queries()
        
        print(f"\n=== INTERSECTING QUERIES ANALYSIS ===")
        print(f"Total Intersecting Queries: {len(intersecting)}")
        
        for query, stats in intersecting.items():
            print(f"\n📊 Query: '{query}'")
            print(f"   • Appears in {stats['frequency']} searches across {stats['unique_prompts']} prompts")
            print(f"   • Total domains with citations: {stats['total_domains']}")
            
            print(f"   • Prompts:")
            for i, prompt in enumerate(stats['prompts'], 1):
                short_prompt = prompt[:80] + "..." if len(prompt) > 80 else prompt
                print(f"     {i}. {short_prompt}")
            
            print(f"   • Top domains across all instances:")
            domain_totals = {}
            for domain, citations in stats['domains_with_citations'].items():
                domain_totals[domain] = len(citations)
            
            # Sort domains by total citations
            sorted_domains = sorted(domain_totals.items(), key=lambda x: x[1], reverse=True)
            for domain, citation_count in sorted_domains[:5]:  # Show top 5
                print(f"     - {domain}: {citation_count} citations")

    def analyze_response_chunks(self, domain_of_interest: str) -> Dict[str, Any]:
        """
        Analyze AI response chunks to understand domain performance in final responses
        """
        chunk_analysis = {}
        
        for prompt, chunks in self.response_chunks.items():
            domain_mentions = []
            competitor_mentions = []
            total_citations = 0
            
            for sentence, domains in chunks.items():
                total_citations += len(domains)
                
                if domain_of_interest in domains:
                    domain_mentions.append({
                        'sentence': sentence,
                        'position': len(domain_mentions) + 1,
                        'citation_count': domains.count(domain_of_interest)
                    })
                else:
                    # Track competitors mentioned in this sentence
                    for domain in domains:
                        competitor_mentions.append({
                            'sentence': sentence,
                            'domain': domain,
                            'position': len(competitor_mentions) + 1
                        })
            
            # Calculate performance metrics
            domain_citations = sum(mention['citation_count'] for mention in domain_mentions)
            competitor_citations = len(competitor_mentions)
            
            chunk_analysis[prompt] = {
                'domain_mentions': domain_mentions,
                'competitor_mentions': competitor_mentions,
                'domain_citations': domain_citations,
                'competitor_citations': competitor_citations,
                'total_citations': total_citations,
                'domain_share': domain_citations / total_citations if total_citations > 0 else 0,
                'first_mention_position': domain_mentions[0]['position'] if domain_mentions else None,
                'performance_rating': self._rate_performance(domain_mentions, competitor_mentions)
            }
        
        return chunk_analysis
    
    def _rate_performance(self, domain_mentions: List[Dict], competitor_mentions: List[Dict]) -> str:
        """Rate domain performance as 'excellent', 'good', 'poor', or 'absent'"""
        if not domain_mentions:
            return 'absent'
        
        first_mention = domain_mentions[0]['position']
        total_citations = sum(mention['citation_count'] for mention in domain_mentions)
        
        if first_mention <= 2 and total_citations >= 2:
            return 'excellent'
        elif first_mention <= 3 and total_citations >= 1:
            return 'good'
        else:
            return 'poor'
    
    def call_gemini_analysis(self, prompt: str, domain_of_interest: str, competitor_info: List[str]) -> str:
        """
        Call Gemini API to analyze why competitors ranked higher
        """
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            return "ERROR: GEMINI_API_KEY not found in environment variables"
        
        # Prepare the analysis prompt for Gemini
        analysis_prompt = f"""
        Analyze why competing domains ranked higher than {domain_of_interest} in AI search results.

        User Query/Prompt: {prompt}

        Competitor information (showing what content made them get cited):
        {chr(10).join(f"- {info}" for info in competitor_info)}

        Based on the competitor content shown above, explain in one sentence what specific content or information the {domain_of_interest} likely lacked that made the competitors preferable citations for this query.

        Focus on actionable insights about content gaps that could help improve {domain_of_interest}'s ranking.
        
        Avoid giving generic advice like "{domain_of_interest} should improve their SEO."
        """
        
        try:
            # Use the same genai library approach as in main.py
            from google import genai
            from google.genai import types
            
            client = genai.Client()
            
            # Configure generation settings (no grounding needed for analysis)
            config = types.GenerateContentConfig(
                max_output_tokens=1024,
                thinking_config=types.ThinkingConfig(
                    thinking_budget=0
                ),  # Disable thinking for speed
            )
            
            response = client.models.generate_content(
                model="gemini-2.5-flash", 
                contents=analysis_prompt, 
                config=config
            )
            
            return response.text
            
        except Exception as e:
            return f"ERROR: Gemini API call failed - {str(e)}"
    
    def analyze_poor_performance(self, domain_of_interest: str) -> Dict[str, Any]:
        """
        Identify the worst 3 prompts where domain performed poorly and get Gemini analysis.
        
        Ranking criteria (worst to best):
        1. Domain was retrieved but got no citations (empty list [])
        2. Domain was cited but the lowest citation number is highest (e.g., [5, 6] is worse than [3, 8])
        """
        # Collect performance data for each prompt
        prompt_performance = []
        
        for prompt, queries in self.data.items():
            domain_appeared = False
            all_citations = []
            
            # Check all queries in this prompt for the target domain
            for query, domains in queries.items():
                if domain_of_interest in domains:
                    domain_appeared = True
                    citations = domains[domain_of_interest]['citations']
                    all_citations.extend(citations)
            
            if domain_appeared:
                if not all_citations:
                    # Domain retrieved but no citations - worst case
                    performance_score = float('inf')
                    min_citation_rank = None
                else:
                    # Domain cited - use minimum citation rank as performance score
                    min_citation_rank = min(all_citations)
                    performance_score = min_citation_rank
                
                prompt_performance.append({
                    'prompt': prompt,
                    'performance_score': performance_score,
                    'min_citation_rank': min_citation_rank,
                    'all_citations': all_citations,
                    'total_citations': len(all_citations)
                })
        
        # Sort by performance score (higher is worse, inf is worst)
        prompt_performance.sort(key=lambda x: x['performance_score'], reverse=True)
        
        # Take the worst 3 prompts
        worst_prompts = prompt_performance[:3]
        
        poor_performance_analysis = {}
        
        for prompt_data in worst_prompts:
            prompt = prompt_data['prompt']
            
            # Get competitor information from this prompt
            competitor_domains = []
            for query, domains in self.data[prompt].items():
                for domain, citation_data in domains.items():
                    if domain != domain_of_interest and citation_data['citations']:
                        # Find competitors that ranked better than our domain
                        if prompt_data['min_citation_rank'] is None or min(citation_data['citations']) < prompt_data['performance_score']:
                            competitor_domains.append({
                                'domain': domain,
                                'query': query,
                                'best_citation': min(citation_data['citations']),
                                'all_citations': citation_data['citations']
                            })
            
            # Create competitor info for Gemini analysis with content instead of ranks
            competitor_info = []
            for comp in competitor_domains[:5]:  # Top 5 competitors
                # Get the content associated with the first citation
                query = comp['query']
                domain = comp['domain']
                
                # Find the content for this competitor
                if query in self.data[prompt] and domain in self.data[prompt][query]:
                    citation_data = self.data[prompt][query][domain]
                    contents = citation_data.get('contents', [])
                    
                    if contents:
                        # Use the first content snippet
                        first_content = contents[0]
                        competitor_info.append(f"Domain {domain} was cited for query '{query}' with content: \"{first_content}\"")
                    else:
                        # Fallback to rank if no content available
                        competitor_info.append(f"Domain {domain} ranked at position {comp['best_citation']} for query: {query}")
                else:
                    # Fallback to rank if domain data not found
                    competitor_info.append(f"Domain {domain} ranked at position {comp['best_citation']} for query: {query}")
            
            if competitor_info:
                gemini_analysis = self.call_gemini_analysis(
                    prompt, 
                    domain_of_interest, 
                    competitor_info
                )
            else:
                gemini_analysis = "No competitor data available for analysis"
            
            # Determine performance rating
            if prompt_data['min_citation_rank'] is None:
                performance_rating = 'absent'
            elif prompt_data['min_citation_rank'] >= 5:
                performance_rating = 'poor'
            else:
                performance_rating = 'suboptimal'
            
            poor_performance_analysis[prompt] = {
                'performance_rating': performance_rating,
                'performance_score': prompt_data['performance_score'],
                'min_citation_rank': prompt_data['min_citation_rank'],
                'total_citations': prompt_data['total_citations'],
                'all_citations': prompt_data['all_citations'],
                'competitor_count': len(competitor_domains),
                'competitor_info': competitor_info,
                'gemini_analysis': gemini_analysis
            }
        
        return poor_performance_analysis
    
    def print_poor_performance_analysis(self, domain_of_interest: str):
        """
        Print detailed analysis of the worst 3 performing prompts with Gemini insights
        """
        poor_performance = self.analyze_poor_performance(domain_of_interest)
        
        if not poor_performance:
            print(f"\n🎉 No poor performance cases found for {domain_of_interest}!")
            return
        
        print(f"\n🔍 WORST 3 PROMPTS ANALYSIS FOR {domain_of_interest}")
        print("=" * 80)
        
        # Sort by performance score to show worst first
        sorted_prompts = sorted(poor_performance.items(), 
                              key=lambda x: x[1]['performance_score'], 
                              reverse=True)
        
        for i, (prompt, analysis) in enumerate(sorted_prompts, 1):
            short_prompt = prompt[:100] + "..." if len(prompt) > 100 else prompt
            print(f"\n📉 #{i} WORST PROMPT: {short_prompt}")
            print(f"   Performance Rating: {analysis['performance_rating'].upper()}")
            
            if analysis['min_citation_rank'] is None:
                print(f"   Citation Status: RETRIEVED BUT NO CITATIONS")
            else:
                print(f"   Best Citation Rank: {analysis['min_citation_rank']}")
                print(f"   All Citations: {analysis['all_citations']}")
            
            print(f"   Total Citations: {analysis['total_citations']}")
            print(f"   Competitors Found: {analysis['competitor_count']}")
            
            print(f"\n   🤖 Gemini Analysis:")
            print(f"   {analysis['gemini_analysis']}")
            
            print(f"\n   🏆 Top Competitor Information:")
            for j, info in enumerate(analysis['competitor_info'][:3], 1):
                print(f"   {j}. {info}")
            print()


def main():
    """
    Main function - checks for experiment results file, otherwise shows demo
    """
    # Check if experiment results file exists (new filename)
    experiment_file = "internal_responce_log.json"
    
    if os.path.exists(experiment_file):
        print(f"📁 Found experiment results file: {experiment_file}")
        print("🔍 Running analysis on experiment data...")
        print("=" * 60)
        
        # Run analysis on the experiment file
        demo_with_experiment_file(experiment_file)
    else:
        print(f"📁 No experiment results file found: {experiment_file}")


if __name__ == "__main__":
    main()
